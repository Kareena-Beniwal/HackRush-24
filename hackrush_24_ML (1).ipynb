{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-04-14 09:33:11.407545: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-14 09:33:11.413854: I external/local_tsl/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2024-04-14 09:33:11.493686: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-04-14 09:33:13.090567: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from natsort import natsorted\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, Flatten, Dense, MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "os.environ['TF_XLA_FLAGS'] = '--tf_xla_enable_xla_devices'\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]= \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]= '2'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X: (73080, 32, 250)\n",
      "Shape of Y: (73080,)\n",
      "Summary Statistics of Y:\n",
      "Min: 1\n",
      "Max: 28\n",
      "Mean: 14.5\n",
      "Median: 14.5\n",
      "Standard Deviation: 8.077747210701755\n",
      "Class Counts:\n",
      "Class 1: 2610 instances\n",
      "Class 2: 2610 instances\n",
      "Class 3: 2610 instances\n",
      "Class 4: 2610 instances\n",
      "Class 5: 2610 instances\n",
      "Class 6: 2610 instances\n",
      "Class 7: 2610 instances\n",
      "Class 8: 2610 instances\n",
      "Class 9: 2610 instances\n",
      "Class 10: 2610 instances\n",
      "Class 11: 2610 instances\n",
      "Class 12: 2610 instances\n",
      "Class 13: 2610 instances\n",
      "Class 14: 2610 instances\n",
      "Class 15: 2610 instances\n",
      "Class 16: 2610 instances\n",
      "Class 17: 2610 instances\n",
      "Class 18: 2610 instances\n",
      "Class 19: 2610 instances\n",
      "Class 20: 2610 instances\n",
      "Class 21: 2610 instances\n",
      "Class 22: 2610 instances\n",
      "Class 23: 2610 instances\n",
      "Class 24: 2610 instances\n",
      "Class 25: 2610 instances\n",
      "Class 26: 2610 instances\n",
      "Class 27: 2610 instances\n",
      "Class 28: 2610 instances\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# # Load the data from the NPZ file\n",
    "data = np.load('data.npz')\n",
    "\n",
    "# Assign X and Y from the loaded data\n",
    "X = data['X']\n",
    "Y = data['Y'].astype(np.int32)\n",
    "\n",
    "# Optionally, check shapes of the loaded arrays\n",
    "print(\"Shape of X:\", X.shape)\n",
    "print(\"Shape of Y:\", Y.shape)\n",
    "\n",
    "# Assuming Y is your numpy array\n",
    "print(\"Summary Statistics of Y:\")\n",
    "print(\"Min:\", np.min(Y))\n",
    "print(\"Max:\", np.max(Y))\n",
    "print(\"Mean:\", np.mean(Y))\n",
    "print(\"Median:\", np.median(Y))\n",
    "print(\"Standard Deviation:\", np.std(Y))\n",
    "\n",
    "# For classification tasks, it might also be useful to see the distribution of classes:\n",
    "if np.issubdtype(Y.dtype, np.integer):  # Check if Y contains integer (class labels typically are integers)\n",
    "    print(\"Class Counts:\")\n",
    "    unique, counts = np.unique(Y, return_counts=True)\n",
    "    for label, count in zip(unique, counts):\n",
    "        print(f\"Class {label}: {count} instances\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remapped Y:\n",
      "[0 0 0 ... 8 8 8]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming Y is your numpy array containing labels\n",
    "label_map = {\n",
    "    1: 0, 2: 0, 3: 0, 4: 1, 5: 1, 6: 1, 7: 2, 8: 2, 9: 2,\n",
    "    10: 3, 11: 3, 12: 3, 13: 4, 14: 4, 15: 4, 16: 6, 17: 5,\n",
    "    18: 5, 19: 5, 20: 6, 21: 6, 22: 6, 23: 7, 24: 7, 25: 7,\n",
    "    26: 8, 27: 8, 28: 8\n",
    "}\n",
    "\n",
    "# Find the maximum label to create a sufficiently large mapping array\n",
    "max_label = max(label_map.keys())\n",
    "\n",
    "# Create an array that will hold the new labels\n",
    "# Initialize it with zeros or another default value that indicates unmapped labels if there are any\n",
    "new_labels = np.zeros(max_label + 1, dtype=int)  # plus 1 because NumPy arrays are 0-indexed\n",
    "\n",
    "# Fill in the new_labels array with mapped values\n",
    "for key, value in label_map.items():\n",
    "    new_labels[key] = value\n",
    "\n",
    "# Remap Y using the new_labels array\n",
    "# Here, Y values must be within the range of label_map keys; otherwise, you need to handle potential out-of-bound indices\n",
    "Y= new_labels[Y]\n",
    "\n",
    "# If you want to check or display the remapped array\n",
    "\n",
    "print(\"Remapped Y:\")\n",
    "print(Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoundCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SoundCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(3, 3), padding=1)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(3, 3), padding=1)\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(3, 3), padding=1)\n",
    "        self.pool = nn.MaxPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(64 * 4 * 31, 512)  # Corrected dimension\n",
    "        self.fc2 = nn.Linear(512, 128)\n",
    "        self.fc3 = nn.Linear(128, 10)  # Assuming 10 different classes\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.relu(self.conv3(x))\n",
    "        x = self.pool(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.relu(self.fc1(x))\n",
    "        x = self.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape data to include channel dimension (1, 32, 250)\n",
    "X = X.reshape(X.shape[0], 1, 32, 250)\n",
    "\n",
    "Y = Y.astype(np.int64)\n",
    "\n",
    "\n",
    "# Convert to torch tensors\n",
    "tensor_X = torch.Tensor(X)  # transform to torch tensor\n",
    "tensor_Y = torch.Tensor(Y)\n",
    "\n",
    "# Create your dataloader\n",
    "my_dataset = TensorDataset(tensor_X, tensor_Y)\n",
    "train_loader = DataLoader(my_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoundCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "\n",
    "# Assuming tensor_X and tensor_Y are already defined\n",
    "dataset = TensorDataset(tensor_X, tensor_Y)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validation_accuracy(model, val_loader):\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    model.train()  # Set the model back to train mode\n",
    "    return 100 * correct / total\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_X = torch.Tensor(X)  # Assuming X is your input data.\n",
    "tensor_Y = torch.Tensor(Y).long()  # Convert labels to long\n",
    "\n",
    "# Create dataset\n",
    "my_dataset = TensorDataset(tensor_X, tensor_Y)\n",
    "train_size = int(0.8 * len(my_dataset))\n",
    "val_size = len(my_dataset) - train_size\n",
    "\n",
    "train_dataset, val_dataset = random_split(my_dataset, [train_size, val_size])\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 2.2036166320222734, Validation Accuracy: 20.997536945812808%\n",
      "Saved better model at epoch 1 with validation accuracy: 20.997536945812808%.\n",
      "Epoch 2, Loss: 2.0586286523670805, Validation Accuracy: 27.68199233716475%\n",
      "Saved better model at epoch 2 with validation accuracy: 27.68199233716475%.\n",
      "Epoch 3, Loss: 1.6360833822246192, Validation Accuracy: 53.99562123700055%\n",
      "Saved better model at epoch 3 with validation accuracy: 53.99562123700055%.\n",
      "Epoch 4, Loss: 0.9584037156804125, Validation Accuracy: 72.45484400656814%\n",
      "Saved better model at epoch 4 with validation accuracy: 72.45484400656814%.\n",
      "Epoch 5, Loss: 0.5507790184614434, Validation Accuracy: 81.1576354679803%\n",
      "Saved better model at epoch 5 with validation accuracy: 81.1576354679803%.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def train_model(model, train_loader, val_loader, criterion, optimizer, num_epochs, save_path):\n",
    "    model.train()\n",
    "    best_accuracy = 0.0  # To track the best model\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for inputs, labels in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "\n",
    "        val_acc = validation_accuracy(model, val_loader)\n",
    "        print(f'Epoch {epoch+1}, Loss: {running_loss / len(train_loader)}, Validation Accuracy: {val_acc}%')\n",
    "        \n",
    "        # Save the best model if the current model is better\n",
    "        if val_acc > best_accuracy:\n",
    "            best_accuracy = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': running_loss,\n",
    "                'accuracy': val_acc\n",
    "            }, save_path)\n",
    "            print(f'Saved better model at epoch {epoch+1} with validation accuracy: {val_acc}%.')\n",
    "\n",
    "# Define model, criterion, optimizer, and file path for saving the model\n",
    "model = SoundCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "save_path = './saveModel.pth'  # Modify path as needed\n",
    "\n",
    "# Call training function\n",
    "train_model(model, train_loader, val_loader, criterion, optimizer, 5, save_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_excel('/home/project/new/Test.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1344192/2964036573.py:24: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:230.)\n",
      "  X_test_blocks = torch.tensor(all_test_blocks, dtype=torch.float)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "# Assuming `test_data` is already loaded and is a Pandas DataFrame\n",
    "all_test_blocks = []\n",
    "block_height = 32\n",
    "num_columns_in_test = 750  # Columns per test block\n",
    "\n",
    "# Iterate over the dataset in chunks of 32 rows\n",
    "for start_row in range(0, test_data.shape[0], block_height):\n",
    "    if start_row + block_height > test_data.shape[0]:\n",
    "        continue  # Skip incomplete blocks\n",
    "    chunk = test_data.iloc[start_row:start_row + block_height]\n",
    "\n",
    "    # Split each 750-column block into three 250-column blocks\n",
    "    for i in range(3):  # Since 750/250 = 3\n",
    "        start_col = i * 250\n",
    "        end_col = start_col + 250\n",
    "        block = chunk.iloc[:, start_col:end_col]\n",
    "        all_test_blocks.append(block.values.reshape(1, block_height, 250))  # Adding channel dimension\n",
    "\n",
    "# Convert blocks to PyTorch tensors\n",
    "X_test_blocks = torch.tensor(all_test_blocks, dtype=torch.float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming the model is already defined as `SoundCNN`\n",
    "model = SoundCNN()\n",
    "model.load_state_dict(torch.load('/home/project/new/saveModel.pth', map_location=device))\n",
    "# model.load_state_dict(torch.load('/home/project/new/SoundCNN_model.pth')['model_state_dict'])\n",
    "\n",
    "# Transfer model to appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set the model to evaluation mode\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_blocks = X_test_blocks.to(device)  # Move the test data to the same device as the model\n",
    "with torch.no_grad():  # Ensure no gradients are computed during inference\n",
    "    predictions = model(X_test_blocks)\n",
    "    predicted_classes = torch.argmax(predictions, dim=1).cpu().numpy()  # Move predictions back to CPU and convert to numpy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# `predicted_classes` is the numpy array containing the class indices predicted by the model\n",
    "predictions_df = pd.DataFrame(predicted_classes, columns=['PredictedClass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path and name of the CSV file\n",
    "csv_file_path = './predictions.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file without the index column\n",
    "predictions_df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_map = {\n",
    "    0: \"Tiger\",\n",
    "    1: \"Snake\",\n",
    "    2: \"Wolf\",\n",
    "    3: \"Bear\",\n",
    "    4: \"Rabbit\",\n",
    "    5: \"Monkey\",\n",
    "    6: \"Eagle\",\n",
    "    7: \"Dolphin\",\n",
    "    8: \"Koala\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Assuming `predicted_classes` is already available as a numpy array\n",
    "predictions_df = pd.DataFrame(predicted_classes, columns=['PredictedClass'])\n",
    "\n",
    "# Replace numeric labels with animal names using the mapping\n",
    "predictions_df['PredictedClass'] = predictions_df['PredictedClass'].replace(label_map)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the path and name of the CSV file\n",
    "csv_file_path = './animal_predictions.csv'\n",
    "\n",
    "# Save the DataFrame to a CSV file without the index column\n",
    "predictions_df.to_csv(csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "# Load predictions\n",
    "predictions_df = pd.read_csv('./animal_predictions.csv')\n",
    "\n",
    "# Function to determine the majority or random in case of a tie\n",
    "def majority_or_random(labels):\n",
    "    count = Counter(labels)\n",
    "    max_freq = max(count.values())\n",
    "    candidates = [label for label, freq in count.items() if freq == max_freq]\n",
    "    return random.choice(candidates)\n",
    "\n",
    "# Group predictions in groups of 3 and apply the function\n",
    "grouped_labels = []\n",
    "for i in range(0, len(predictions_df), 3):\n",
    "    labels = predictions_df['PredictedClass'][i:i+3].tolist()\n",
    "    if len(labels) == 3:  # Ensure it's a full group of 3\n",
    "        grouped_label = majority_or_random(labels)\n",
    "        grouped_labels.append(grouped_label)\n",
    "\n",
    "# Create new DataFrame\n",
    "final_predictions_df = pd.DataFrame(grouped_labels, columns=['MajorityVotedClass'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "final_csv_file_path = './final_animal_predictions.csv'\n",
    "final_predictions_df.to_csv(final_csv_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the CSV file\n",
    "file_path = './final_animal_predictions.csv'\n",
    "predictions_df = pd.read_csv(file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the 'ID' column\n",
    "predictions_df['ID'] = ['id_' + str(index + 1) for index in predictions_df.index]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reorder columns to make 'ID' the first column\n",
    "column_order = ['ID', 'MajorityVotedClass']\n",
    "predictions_df = predictions_df[column_order]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated DataFrame to a new CSV file\n",
    "updated_csv_file_path = './updated_final_animal_predictions.csv'\n",
    "predictions_df.to_csv(updated_csv_file_path, index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "to19",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
